{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1fbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Target: data/nnUNet_raw/Dataset501_BrainMets\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- GLOBAL CONFIGURATION ---\n",
    "DATASET_NAME = \"Dataset501_BrainMets\"  # Must follow 'DatasetXXX_Name' format\n",
    "SOURCE_DIR = 'data/brainmetshare-3'\n",
    "OUTPUT_DIR = os.getenv('nnUNet_raw', \"data/nnUNet_raw\")\n",
    "\n",
    "# Mapping modalities to nnU-Net suffixes\n",
    "# 0000: T1 Gd, 0001: T1 Pre, 0002: FLAIR, 0003: BRAVO\n",
    "MODALITY_MAP = {\n",
    "    \"t1_gd.nii.gz\": \"0000\",\n",
    "    \"t1_pre.nii.gz\": \"0001\",\n",
    "    \"flair.nii.gz\": \"0002\",\n",
    "    \"bravo.nii.gz\": \"0003\"\n",
    "}\n",
    "\n",
    "# Define Paths\n",
    "target_base = Path(OUTPUT_DIR) / DATASET_NAME\n",
    "imagesTr = target_base / \"imagesTr\"\n",
    "labelsTr = target_base / \"labelsTr\"\n",
    "imagesTs = target_base / \"imagesTs\"\n",
    "\n",
    "# Create directories\n",
    "for p in [imagesTr, labelsTr, imagesTs]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Setup complete. Target: {target_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ae45c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Dataset Audit: data/brainmetshare-3 ---\n",
      "\n",
      "Audit Finished.\n",
      "Total Cases: 156\n",
      "Clean Cases: 156\n",
      "Cases with Issues: 0\n",
      "\n",
      "All cases passed geometry and modality checks!\n"
     ]
    }
   ],
   "source": [
    "# --- VALIDATION CONFIGURATION ---\n",
    "CHECK_REGISTRATION = True  # Compares affine matrices and shapes\n",
    "REQ_MODALITIES = list(MODALITY_MAP.keys())\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"--- Starting Dataset Audit: {SOURCE_DIR} ---\")\n",
    "\n",
    "for subset in ['train', 'test']:\n",
    "    subset_path = Path(SOURCE_DIR) / subset\n",
    "    if not subset_path.exists():\n",
    "        continue\n",
    "\n",
    "    cases = [d for d in subset_path.iterdir() if d.is_dir()]\n",
    "\n",
    "    for case_dir in cases:\n",
    "        case_id = case_dir.name\n",
    "        report = {\"case_id\": case_id, \"subset\": subset,\n",
    "                  \"missing\": [], \"issue\": \"None\"}\n",
    "\n",
    "        # 1. Check for Missing Files\n",
    "        found_files = [f.name for f in case_dir.glob(\"*.nii.gz\")]\n",
    "        for req in REQ_MODALITIES:\n",
    "            if req not in found_files:\n",
    "                report[\"missing\"].append(req)\n",
    "\n",
    "        # 2. Check Segmentation (for train)\n",
    "        if subset == 'train' and \"seg.nii.gz\" not in found_files:\n",
    "            report[\"issue\"] = \"Missing Label\"\n",
    "\n",
    "        # 3. Registration & Geometry Check\n",
    "        if CHECK_REGISTRATION and not report[\"missing\"]:\n",
    "            try:\n",
    "                # Load first modality as reference\n",
    "                ref_path = case_dir / REQ_MODALITIES[0]\n",
    "                ref_img = nib.load(str(ref_path))\n",
    "                ref_affine = ref_img.affine\n",
    "                ref_shape = ref_img.shape\n",
    "\n",
    "                for other in REQ_MODALITIES[1:]:\n",
    "                    other_img = nib.load(str(case_dir / other))\n",
    "\n",
    "                    # Check if shapes match\n",
    "                    if not np.array_equal(other_img.shape, ref_shape):\n",
    "                        report[\"issue\"] = f\"Shape Mismatch ({other})\"\n",
    "                        break\n",
    "\n",
    "                    # Check if affines match (tolerance for floating point)\n",
    "                    if not np.allclose(other_img.affine, ref_affine, atol=1e-3):\n",
    "                        report[\"issue\"] = f\"Registration Drift ({other})\"\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                report[\"issue\"] = f\"Error loading NIfTI: {str(e)}\"\n",
    "\n",
    "        results.append(report)\n",
    "\n",
    "# --- Summary Report ---\n",
    "df_audit = pd.DataFrame(results)\n",
    "issues_df = df_audit[(df_audit['missing'].str.len() > 0)\n",
    "                     | (df_audit['issue'] != \"None\")]\n",
    "\n",
    "print(f\"\\nAudit Finished.\")\n",
    "print(f\"Total Cases: {len(df_audit)}\")\n",
    "print(f\"Clean Cases: {len(df_audit) - len(issues_df)}\")\n",
    "print(f\"Cases with Issues: {len(issues_df)}\")\n",
    "\n",
    "if not issues_df.empty:\n",
    "    print(\"\\n--- Summary of Pitfalls Found ---\")\n",
    "    print(issues_df[['case_id', 'subset', 'missing', 'issue']].to_string())\n",
    "else:\n",
    "    print(\"\\nAll cases passed geometry and modality checks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2bc085f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train (105 cases)...\n",
      "Processing test (51 cases)...\n",
      "File copying complete.\n"
     ]
    }
   ],
   "source": [
    "def convert_subset(subset_name, target_img_dir, target_lab_dir=None):\n",
    "    subset_path = Path(SOURCE_DIR) / subset_name\n",
    "    case_dirs = [d for d in subset_path.iterdir() if d.is_dir()]\n",
    "\n",
    "    print(f\"Processing {subset_name} ({len(case_dirs)} cases)...\")\n",
    "\n",
    "    for case_dir in case_dirs:\n",
    "        case_id = case_dir.name  # e.g., Mets_005\n",
    "\n",
    "        # 1. Process Images\n",
    "        for filename, suffix in MODALITY_MAP.items():\n",
    "            src_file = case_dir / filename\n",
    "            if src_file.exists():\n",
    "                # nnU-Net format: CaseName_000X.nii.gz\n",
    "                dest_file = target_img_dir / f\"{case_id}_{suffix}.nii.gz\"\n",
    "                shutil.copy(src_file, dest_file)\n",
    "\n",
    "        # 2. Process Segmentation (Labels) - only if in train and file exists\n",
    "        if target_lab_dir:\n",
    "            seg_file = case_dir / \"seg.nii.gz\"\n",
    "            if seg_file.exists():\n",
    "                dest_seg = target_lab_dir / f\"{case_id}.nii.gz\"\n",
    "                shutil.copy(seg_file, dest_seg)\n",
    "\n",
    "\n",
    "# Execute conversion\n",
    "convert_subset(\"train\", imagesTr, labelsTr)\n",
    "convert_subset(\"test\", imagesTs, None)\n",
    "\n",
    "print(\"File copying complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842f4e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.json generated at data/nnUNet_raw/Dataset501_BrainMets/dataset.json\n"
     ]
    }
   ],
   "source": [
    "dataset_json = {\n",
    "    \"channel_names\": {\n",
    "        \"0\": \"T1_Gd\",\n",
    "        \"1\": \"T1_pre\",\n",
    "        \"2\": \"FLAIR\",\n",
    "        \"3\": \"BRAVO\"\n",
    "    },\n",
    "    \"labels\": {\n",
    "        \"background\": 0,\n",
    "        \"metastasis\": 1\n",
    "    },\n",
    "    \"numTraining\": len(list(labelsTr.glob(\"*.nii.gz\"))),\n",
    "    \"file_ending\": \".nii.gz\",\n",
    "    \"overwrite_image_reader_writer\": \"NibabelIOWithCast\"  # Optional, standard for NIfTI\n",
    "}\n",
    "\n",
    "with open(target_base / \"dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset_json, f, indent=4)\n",
    "\n",
    "print(f\"dataset.json generated at {target_base / 'dataset.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c827ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Dataset501_BrainMets:\n",
      "Training images: 105\n",
      "Training labels: 105\n",
      "Test images:     51\n",
      "data/nnUNet_raw/Dataset501_BrainMets:\n",
      "dataset.json\n",
      "imagesTr\n",
      "imagesTs\n",
      "labelsTr\n",
      "\n",
      "data/nnUNet_raw/Dataset501_BrainMets/imagesTr:\n",
      "Mets_005_0000.nii.gz\n",
      "Mets_005_0001.nii.gz\n",
      "Mets_005_0002.nii.gz\n",
      "Mets_005_0003.nii.gz\n",
      "Mets_010_0000.nii.gz\n",
      "Mets_010_0001.nii.gz\n",
      "Mets_010_0002.nii.gz\n",
      "Mets_010_0003.nii.gz\n",
      "Mets_011_0000.nii.gz\n",
      "Mets_011_0001.nii.gz\n",
      "Mets_011_0002.nii.gz\n",
      "Mets_011_0003.nii.gz\n",
      "Mets_013_0000.nii.gz\n"
     ]
    }
   ],
   "source": [
    "print(f\"Summary for {DATASET_NAME}:\")\n",
    "print(f\"Training images: {len(list(imagesTr.glob('*_0000.nii.gz')))}\")\n",
    "print(f\"Training labels: {len(list(labelsTr.glob('*.nii.gz')))}\")\n",
    "print(f\"Test images:     {len(list(imagesTs.glob('*_0000.nii.gz')))}\")\n",
    "\n",
    "# Final directory structure check\n",
    "!ls -R {target_base} | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d67402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Dataset Audit: data/brainmetshare-3 ---\n",
      "\n",
      "Audit Finished.\n",
      "Total Cases: 156\n",
      "Clean Cases: 156\n",
      "Cases with Issues: 0\n",
      "\n",
      "All cases passed geometry and modality checks!\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnunetv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
